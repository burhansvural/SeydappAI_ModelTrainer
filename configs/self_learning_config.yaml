# configs/self_learning_config.yaml
# âœ… StarCoder2-3B Optimized Self-Learning Configuration
# Based on bigcode/starcoder2-3b specifications

name: seydappai-starcoder2-self-learning
version: 1.0.0
schema: v1

# ğŸ¤– Model Configuration - StarCoder2-3B Specific
model:
  name: "bigcode/starcoder2-3b"
  type: "code_generation"
  parameters: "3B"
  context_window: 16384        # âœ… StarCoder2-3B context window[4]
  sliding_window: 4096         # âœ… Sliding window attention[4]
  vocab_size: 49152           # âœ… StarCoder2 vocabulary size[1]
  hidden_size: 3072           # âœ… Hidden dimension[1]
  programming_languages: 17    # âœ… Supported languages[4]

  # âœ… Training Objective Configuration
  training_objective: "fill_in_the_middle"  # âœ… StarCoder2 FIM objective[4]
  max_position_embeddings: 4096             # âœ… From StarCoder2Config[1]

# ğŸ§  Self-Learning Core Configuration
self_learning:

  # ğŸ” Research Configuration - Enhanced
  research:
    search_engines: ["google", "bing", "duckduckgo"]
    content_types:
      - "documentation"
      - "stackoverflow"
      - "github"
      - "arxiv"
      - "medium"
      - "dev.to"
    daily_research_limit: 200               # âœ… Increased for more knowledge
    research_topics:
      - "python async programming"
      - "pytorch optimization techniques"
      - "transformer fine-tuning methods"
      - "neural network training best practices"
      - "code generation with transformers"
      - "starcoder model optimization"
      - "lora adapter training"
    research_quality_threshold: 0.75
    max_content_length: 5000               # âœ… Content length limit
    research_interval_minutes: 30          # âœ… Every 30 minutes

  # ğŸš€ Training Configuration - Optimized for RTX 3060
  training:
    confidence_threshold: 0.80             # âœ… Lowered from 0.85
    knowledge_threshold: 2                 # âœ… FIXED: Reduced from 5 to 2
    retrain_interval_hours: 2              # âœ… More frequent training
    max_new_samples_per_cycle: 25          # âœ… Optimized for 12GB VRAM

    # âœ… StarCoder2-3B Specific Training Parameters
    learning_rate: 2e-5                    # âœ… Optimized for code models
    batch_size: 1                          # âœ… RTX 3060 12GB optimized
    gradient_accumulation_steps: 4         # âœ… Effective batch size = 8
    max_steps: 100                         # âœ… Incremental training steps
    warmup_steps: 10                       # âœ… 10% warmup
    weight_decay: 0.01
    max_grad_norm: 1.0

    # âœ… LoRA Configuration for StarCoder2-3B
    lora:
      r: 16                                # âœ… LoRA rank
      alpha: 32                            # âœ… LoRA alpha (2*r)
      dropout: 0.1                         # âœ… LoRA dropout
      target_modules:
        - "q_proj"
        - "k_proj"
        - "v_proj"
        - "o_proj"
        - "up_proj"
        - "down_proj"
        - "gate_proj"
      bias: "none"
      task_type: "CAUSAL_LM"

  # ğŸ§  Knowledge Graph Configuration - Enhanced
  knowledge_graph:
    embedding_model: "all-MiniLM-L6-v2"   # âœ… Lightweight for RTX 3060
    max_knowledge_nodes: 50000             # âœ… Increased capacity
    similarity_threshold: 0.7              # âœ… Entity similarity
    entity_types:
      - "LANG"          # Programming languages
      - "CONCEPT"       # Programming concepts
      - "FUNCTION"      # Function names
      - "CLASS"         # Class names
      - "LIBRARY"       # Libraries/frameworks
      - "PATTERN"       # Design patterns
      - "ERROR"         # Error types
      - "SYNTAX"        # Syntax elements

    # âœ… Knowledge Persistence
    persistence:
      save_interval_minutes: 5             # âœ… Auto-save every 5 minutes
      backup_interval_hours: 24            # âœ… Daily backups
      max_backup_files: 7                  # âœ… Keep 1 week of backups
      compression: true                    # âœ… Compress knowledge files

  # ğŸ¯ Autonomous Learning Configuration
  autonomous:
    enabled: true                          # âœ… Enable autonomous learning
    evaluation_interval_minutes: 5        # âœ… Check every 5 minutes
    force_training_after_hours: 4          # âœ… Force training if no activity

    # âœ… Learning Triggers
    triggers:
      knowledge_growth_rate: 0.1           # âœ… 10% growth triggers training
      confidence_drop_threshold: 0.1       # âœ… Confidence drop triggers research
      error_rate_threshold: 0.3            # âœ… High error rate triggers retraining

    # âœ… Quality Control
    quality_control:
      min_training_samples: 5              # âœ… Minimum samples for training
      max_training_time_minutes: 30        # âœ… Training timeout
      validation_split: 0.2                # âœ… 20% validation split
      early_stopping_patience: 3           # âœ… Early stopping

# ğŸ› ï¸ Hardware Optimization - RTX 3060 Specific
hardware:
  device: "cuda"
  memory_optimization: true
  gradient_checkpointing: true             # âœ… Save VRAM
  fp16: true                              # âœ… Mixed precision
  dataloader_pin_memory: true             # âœ… Faster data loading
  dataloader_num_workers: 4               # âœ… Parallel data loading

  # âœ… VRAM Management
  vram_management:
    max_memory_fraction: 0.85             # âœ… Use 85% of 12GB
    clear_cache_interval: 10              # âœ… Clear every 10 steps
    offload_optimizer: false              # âœ… Keep optimizer on GPU for 3B model

# ğŸ“Š Monitoring and Logging
monitoring:
  log_level: "INFO"
  log_interval_steps: 5                   # âœ… Log every 5 steps
  save_steps: 25                          # âœ… Save checkpoint every 25 steps
  eval_steps: 25                          # âœ… Evaluate every 25 steps

  # âœ… Metrics Tracking
  metrics:
    - "loss"
    - "perplexity"
    - "learning_rate"
    - "gradient_norm"
    - "memory_usage"
    - "tokens_per_second"

  # âœ… UI Update Configuration
  ui_updates:
    refresh_interval_seconds: 2           # âœ… Real-time UI updates
    terminal_max_lines: 100               # âœ… Terminal log limit
    progress_update_frequency: "step"     # âœ… Update every step

# ğŸ”’ Safety and Validation
safety:
  content_filtering:
    enabled: true
    profanity_filter: true
    code_injection_protection: true
    max_response_length: 2048

  model_validation:
    validate_before_training: true
    validate_after_training: true
    safety_checks: ["coherence", "factuality", "safety"]

# ğŸš€ Performance Tuning - StarCoder2-3B Optimized
performance:
  compilation:
    torch_compile: false                  # âœ… Disable for stability
    compile_mode: "default"

  memory_efficient_attention: true       # âœ… Flash attention if available
  use_reentrant_checkpoint: false        # âœ… Modern gradient checkpointing

  # âœ… Inference Optimization
  inference:
    do_sample: true
    temperature: 0.7
    top_p: 0.9
    top_k: 50
    repetition_penalty: 1.1
    max_new_tokens: 512                   # âœ… Code generation length

# ğŸ”„ Data Pipeline Configuration
data_pipeline:
  preprocessing:
    tokenizer_parallelism: true
    remove_duplicates: true
    shuffle_buffer_size: 1000

  # âœ… Code-Specific Processing
  code_processing:
    preserve_indentation: true
    remove_comments: false                # âœ… Keep comments for context
    max_line_length: 100
    supported_extensions:
      - ".py"
      - ".js"
      - ".ts"
      - ".java"
      - ".cpp"
      - ".c"
      - ".rs"
      - ".go"
      - ".php"
      - ".rb"
      - ".swift"
      - ".kt"
      - ".scala"
      - ".r"
      - ".sql"
      - ".html"
      - ".css"

# ğŸ§ª Experimental Features
experimental:
  multi_modal_learning: false            # âœ… Future: Code + documentation
  adaptive_learning_rate: true           # âœ… Dynamic LR adjustment
  curriculum_learning: false             # âœ… Future: Progressive difficulty
  meta_learning: false                   # âœ… Future: Learn to learn
