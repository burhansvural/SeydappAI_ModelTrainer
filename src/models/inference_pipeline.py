import torch
from transformers import pipeline
from .model_loader import get_model_loader
import logging

logger = logging.getLogger(__name__)


class InferencePipeline:
    _instance = None
    _pipeline = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(InferencePipeline, cls).__new__(cls)
            cls._instance._initialize()
        return cls._instance

    def _initialize(self):
        """
        Modeli ve tokenizer'Ä± yÃ¼kleyip text-generation pipeline'Ä±nÄ± kurar.
        Bu iÅŸlem sadece ilk Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nda bir kez yapÄ±lÄ±r (Singleton).
        """
        if self._pipeline is not None:
            logger.info("Inference pipeline zaten mevcut.")
            return

        try:
            logger.info("ğŸ¤– Inference pipeline baÅŸlatÄ±lÄ±yor... Model yÃ¼kleniyor...")
            model_loader = get_model_loader()

            # Mevcut geliÅŸmiÅŸ loader'Ä±nÄ±zÄ± kullanarak modeli yÃ¼kleyin
            # Model adÄ±nÄ± varsayÄ±lan olarak ayarlÄ±yoruz, ileride deÄŸiÅŸtirilebilir.
            model, tokenizer = model_loader.load_base_model(
                model_name="bigcode/starcoder2-3b",
                use_quantization=True,
                use_cache=True
            )

            if model is None or tokenizer is None:
                raise RuntimeError("Model veya tokenizer yÃ¼klenemedi.")

            self._pipeline = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device_map="auto",
                max_new_tokens=1024,
                repetition_penalty=1.1
            )
            logger.info("âœ… Inference pipeline baÅŸarÄ±yla kuruldu.")
        except Exception as e:
            logger.error(f"âŒ Inference pipeline kurulumunda kritik hata: {e}", exc_info=True)
            self._pipeline = None

    def generate(self, prompt: str) -> str:
        """
        Verilen prompt'a gÃ¶re yapay zekadan bir cevap Ã¼retir.
        """
        if self._pipeline is None:
            logger.error("âŒ Pipeline kurulu deÄŸil. Cevap Ã¼retilemiyor.")
            return "Yapay zeka modeli ÅŸu anda kullanÄ±lamÄ±yor. LÃ¼tfen loglarÄ± kontrol edin."

        try:
            logger.info("ğŸ§  Yapay zekadan cevap Ã¼retiliyor...")

            # StarCoder2 iÃ§in Ã¶nerilen prompt formatÄ±
            formatted_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>"

            sequences = self._pipeline(
                formatted_prompt,
                do_sample=True,
                top_k=10,
                num_return_sequences=1,
                eos_token_id=self._pipeline.tokenizer.eos_token_id,
            )

            generated_text = sequences[0]['generated_text']

            # Prompt'u temizleyip sadece asistanÄ±n cevabÄ±nÄ± al
            assistant_response = generated_text.split("<|assistant|>")[-1].strip()

            logger.info("âœ… Cevap baÅŸarÄ±yla Ã¼retildi.")
            return assistant_response

        except Exception as e:
            logger.error(f"âŒ Cevap Ã¼retimi sÄ±rasÄ±nda hata: {e}", exc_info=True)
            return f"Cevap Ã¼retilirken bir hata oluÅŸtu: {str(e)}"


# Global eriÅŸim iÃ§in bir fonksiyon
def get_inference_pipeline() -> InferencePipeline:
    return InferencePipeline()